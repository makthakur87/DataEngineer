{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73c597c6-158c-4e2a-be40-2583b3301e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Input Data\n",
    "\n",
    "![](Images/72/72 Input Data.jpg)\n",
    "\n",
    "### Solution\n",
    "\n",
    "![](Images/72/72 Solution.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2f59280-a83e-4f54-9782-f9742f0eaac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario\n",
    "### Lets assume we have duplicate records in our dataset based on particular key. We need to remove duplicate records but at the same time need to consider the maximum value fo each column among duplicate values. To develop a logic for this requirement, below are the steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e714ec-86b3-45fd-8f2d-7d94c0136c23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc269069-f340-4009-9634-ad32f2ce7924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "simpleData = (\n",
    "  (100, \"Mobile\", 5000, 10),\n",
    "  (100, \"Mobile\", 7000, 7),\n",
    "  (200, \"Laptop\", 20000, 4),\n",
    "  (200, \"Laptop\", 25000, 8),\n",
    "  (200, \"Laptop\", 22000, 12)\n",
    ")\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "defSchema = StructType(\n",
    "  [StructField(\"Product_ID\", IntegerType(), False),\n",
    "   StructField(\"Product_Name\", StringType(), True),\n",
    "   StructField(\"Price\", IntegerType(), True),\n",
    "   StructField(\"DiscountPercent\", IntegerType(), True)\n",
    "  ])\n",
    "\n",
    "df = spark.createDataFrame(data = simpleData, schema = defSchema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "339d12d1-a9fd-450f-a33c-049938838cc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Max Over Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d1480da-acf6-4106-b91c-e996b03b5267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import max, col\n",
    "\n",
    "windowSpec = Window.partitionBy(\"Product_ID\")\n",
    "dfMax = (\n",
    "            df.withColumn(\"maxPrice\", max(\"Price\").over(windowSpec))\n",
    "            .withColumn(\"maxDiscountPercent\", max(\"DiscountPercent\").over(windowSpec))\n",
    "         )\n",
    "        #  .withColumn(\"maxProduct_Name\", max(\"Product_Name\").over(windowSpec))\n",
    "\n",
    "display(dfMax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6237d418-9469-466a-9921-3dfb21e361f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Select Max Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e83ad5a7-d2f7-4ed5-8aa8-f9701209d237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfSel = dfMax.select(col(\"Product_ID\"), col(\"Product_Name\"), col(\"maxPrice\").alias(\"Price\"), col(\"maxDiscountPercent\").alias(\"DiscountPercent\"))\n",
    "display(dfSel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dfb1eeb-4534-4aaf-b978-6ac41263f6b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde4d85b-1a60-4c4a-9118-c2fa656de491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfOut = dfSel.dropDuplicates()\n",
    "display(dfOut)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fff17d81-63d2-4cda-80e0-4801a7d591e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Complete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55b8fd67-0b06-40b2-973f-0453ec3d922a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import max, col\n",
    "\n",
    "windowSpec = Window.partitionBy(\"Product_ID\")\n",
    "dfMax = (\n",
    "            df.withColumn(\"maxPrice\", max(\"Price\").over(windowSpec))\n",
    "            .withColumn(\"maxDiscountPercent\", max(\"DiscountPercent\").over(windowSpec))\n",
    "         )\n",
    "\n",
    "dfSel = dfMax.select(col(\"Product_ID\"), col(\"Product_Name\"), col(\"maxPrice\").alias(\"Price\"), col(\"maxDiscountPercent\").alias(\"DiscountPercent\"))\n",
    "\n",
    "dfOut = dfSel.dropDuplicates()\n",
    "display(dfOut)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "72 - Get Only Maximum Value of Each Column among Duplicate Records",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
