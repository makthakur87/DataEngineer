{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc68b7c4-7222-4728-aad8-35886ddc4df4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Sample Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e6375a-fadd-4f6d-bff9-d88d2f30e06c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# define the schema for the dataframe\n",
    "schema = StructType([\n",
    "    StructField('ProductID', IntegerType(), True),\n",
    "    StructField('ProductName', StringType(), True),\n",
    "    StructField('ProductCategory', StringType(), True),\n",
    "    StructField('ProductPrice', DoubleType(), True),\n",
    "    StructField('ProductQuantity', IntegerType(), True)\n",
    "    ])\n",
    "# create a list of rows\n",
    "data =[\n",
    "  (1, 'Laptop', 'Electronics', 999.99, 50),\n",
    "  (2, 'Smartphone', 'Electronics', 699.99, 100),\n",
    "  (3, 'Headphones', 'Electronics', 49.99, 200),\n",
    "  (4, 'Book', 'Books', 19.99, 300),\n",
    "  (5, 'Tablet', 'Electronics', 299.99, 75)\n",
    "]\n",
    "\n",
    "# create dataframe\n",
    "productDF = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# show dataframe\n",
    "display(productDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1321f86a-6d80-49d1-8fcd-7fa3e19fba57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Old Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dfa850b-8216-4cc6-b234-357f02cc737b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "productDF.createOrReplaceTempView(\"v_product\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b475c681-07c0-43e9-8953-169b6c0775c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from v_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "031259ad-bc2f-412e-858b-d4512e46f288",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### New Approach\n",
    "\n",
    "### Query Dataframe using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baa1ecf5-5a83-46d5-976d-fc9c791ae1be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"SELECT * FROM {table}\", table=productDF)\n",
    "display(sqlDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4955ed89-1176-459f-89c2-d8d88dc8522d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sqlDF = spark.sql('select {column} from {table}', column=productDF['ProductName'], table=productDF)\n",
    "display(sqlDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e4df16d-e369-4358-bea7-c79f304e1fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transform Dataframe using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4a81a73-0b08-46c2-acd4-ac8d3342ebcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trasnformDF = spark.sql('select ProductID, concat(ProductName, ProductCategory), ProductPrice * ProductQuantity as Total_Cost from {table}', table=productDF)\n",
    "display(trasnformDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a48afbec-f200-43b4-8cfc-0c6b5822bfe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transform Dataframe using Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e5d369-4b46-421c-8ed5-d355ab46c8b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, expr\n",
    "\n",
    "transformDFNew = productDF.select(col('ProductID'), concat(col('ProductName'), col('ProductCategory')).alias('ProductNameCategory'), expr('ProductPrice * ProductQuantity').alias('Total_Cost'))\n",
    "display(transformDFNew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5e57c8d-0371-460c-b84b-78d3bf90248a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Join Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a810946a-d397-4f5e-b833-5f785f285045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Product and Sales Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4978aaf3-d7ad-4517-a916-f8a866887892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# sample data for products\n",
    "product_Data = [\n",
    "    Row(Product_Id = 1, Product_Name = 'Laptop', Product_Price = 800),\n",
    "    Row(Product_Id = 2, Product_Name = 'Smartphone', Product_Price = 500),\n",
    "    Row(Product_Id = 3, Product_Name = 'Tablet', Product_Price = 300),\n",
    "    Row(Product_Id = 4, Product_Name = 'Desktop', Product_Price = 1000),\n",
    "    Row(Product_Id = 5, Product_Name = 'Printer', Product_Price = 200),\n",
    "]\n",
    "\n",
    "# sample data for sales\n",
    "sales_Data =[\n",
    "    Row(Sale_Id = 101, Product_Id = 1, Quantity = 5),\n",
    "    Row(Sale_Id = 102, Product_Id = 2, Quantity = 8),\n",
    "    Row(Sale_Id = 103, Product_Id = 1, Quantity = 3),\n",
    "    Row(Sale_Id = 104, Product_Id = 3, Quantity = 6),\n",
    "    Row(Sale_Id = 105, Product_Id = 4, Quantity = 2),\n",
    "    Row(Sale_Id = 106, Product_Id = 1, Quantity = 7) \n",
    "]\n",
    "\n",
    "# create dataframe for products and sales\n",
    "product_df = spark.createDataFrame(product_Data)\n",
    "sales_df = spark.createDataFrame(sales_Data)\n",
    "\n",
    "# show dataframes\n",
    "product_df.display()\n",
    "sales_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b3cd5e2-02db-4645-ba46-387a48b5f38f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Join Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd10c60e-0636-4113-a081-4ef601d2744c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joinDF = spark.sql('select * from {table1} a join {table2} b on a.{joiningKey} = b.{joiningKey}', table1=product_df, table2=sales_df, joiningKey=product_df['Product_Id'])\n",
    "joinDF.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8929678433222789,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "102 - QueryFrame Using Spark SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
