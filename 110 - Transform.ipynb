{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db5c681c-f732-4008-a249-5ea1c10bde07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What is Transform??\n",
    "\n",
    "![](Images/110/110 Transform.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94ddf5c1-83ee-45b8-816d-843e773acda6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Advantages\n",
    "\n",
    "![](Images/110/110 Advantages.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "523c906a-5d63-4b00-a080-5d72732b8365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Use Cases\n",
    "\n",
    "![](Images/110/110 Use Cases.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbb122d5-cae8-4003-ac90-fef282d354ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Sample Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c22e95c-0f9b-4a49-a9a1-629a1371d84d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sample dataframe\n",
    "data = [\n",
    "    ('Product1', 100, 'Category1'),\n",
    "    ('Product2', 200, 'Category2'),\n",
    "    ('Product3', 300, 'Category1'),\n",
    "]\n",
    "\n",
    "columns = ['ProdcutName', 'Price', 'Category']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "341fb24f-cced-4525-b2e3-d5c1aa3655ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transform Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de10318b-f16e-4c0d-a1e1-f61ebb71adbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define transformation function\n",
    "\n",
    "def transform_function(df):\n",
    "    return df.withColumn('DiscountedPrice', df.Price * 0.9)\n",
    "\n",
    "# apply transform funciton using transform method\n",
    "transformed_df = df.transform(transform_function)\n",
    "transformed_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baf6a038-46dd-49ea-88fc-20b717454e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transform with Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9619cf88-399d-467a-8653-6b992bbd1f32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# sample dataframe\n",
    "data = [\n",
    "    ('Product1', 100, 'Category1'),\n",
    "    ('Product2', 200, 'Category2'),\n",
    "    ('Product3', 300, 'Category1'),\n",
    "]\n",
    "\n",
    "columns = ['ProdcutName', 'Price', 'Category']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()\n",
    "\n",
    "# define transformation function\n",
    "\n",
    "def transform_function(df, discounted_percentage):\n",
    "    return df.withColumn('DiscountedPrice', col('Price') * (1 - discounted_percentage / 100))\n",
    "  \n",
    "# parameterized discount percentage\n",
    "percentage = 20\n",
    "\n",
    "# apply transform funciton using transform method\n",
    "transformed_df = df.transform(transform_function, percentage)\n",
    "transformed_df2 = df.transform(transform_function, discounted_percentage=percentage)\n",
    "transformed_df3 = df.transform(lambda df: transform_function(df, percentage))\n",
    "transformed_df.display()\n",
    "transformed_df2.display()\n",
    "transformed_df3.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef13cd65-d8f9-4860-a64a-e90a8b93fa8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Comparison without Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5aa47d7-ab00-41e1-8bbd-8dfd3bfaf685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, year, month, length, abs, when, last_day, date_format, expr\n",
    "\n",
    "# sample dataframe\n",
    "data = [\n",
    "    ('TXN001', '2023-02-18', 250.75, 'Electronics', 'Bought a new phone'),\n",
    "    ('TXN002', '2024-02-25', -50.50, 'Groceries', 'Refunded groceries'),\n",
    "    ('TXN003', '2019-03-01', 125.00, 'Clothing', 'Purchased new Jacket'),\n",
    "    ('TXN004', '2024-11-28', -10.00, 'Books', 'Refunded Book Purchase'),\n",
    "]\n",
    "\n",
    "# schema definition\n",
    "schema = 'TransactionID String, TransactionDate String, Amount Double, Category String, Description String'\n",
    "\n",
    "# create dataframe\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.display()\n",
    "\n",
    "# apply transformation\n",
    "df = (df\n",
    "      .withColumn('TransactionDate', to_date(col('TransactionDate'), 'yyyy-MM-dd')) # 1. convert to date\n",
    "      .withColumn('Year', year(col('TransactionDate'))) # 2. extract year\n",
    "      .withColumn('Month', month(col('TransactionDate'))) # 3. extract month\n",
    "      .withColumn('Description_Length', length(col('Description'))) # 4. lengh of the description\n",
    "      .withColumn('Amount_Abs', abs(col('Amount'))) # 5. absolute value of amount\n",
    "      .withColumn('Is_Refund', when(col('Amount') < 0, True).otherwise(False)) # 6. indicate if refund\n",
    "      .withColumn('Last_Day_OfMonth', last_day(col('TransactionDate'))) # 7. last day of the month\n",
    "      .withColumn('Formatted_Date', date_format(col('TransactionDate'), 'yyyy-MM')) # 8. format date\n",
    "      .withColumn('Transaction_Size', when(col('Amount_Abs') > 100, 'Large').otherwise('small')) # 9. categorize transaction size\n",
    "      .withColumn('Dynamic_Calculation',\n",
    "                  expr(\n",
    "                        \"\"\"\n",
    "                            CASE\n",
    "                                WHEN Category = 'Electronics' THEN Amount * 1.10\n",
    "                                WHEN Category = 'Groceries' THEN Amount * 0.90\n",
    "                                ELSE Amount\n",
    "                            END\n",
    "                        \"\"\" \n",
    "                       ) # 10. dynamic calculation based on category\n",
    "                  )\n",
    ")\n",
    "\n",
    "# show the result\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef154edc-a852-45e4-9b90-9fdcc94a5925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Comparison with Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c3d1416-d294-4c9f-ba56-b8614474bb11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, year, month, length, abs, when, last_day, date_format, expr\n",
    "\n",
    "# sample dataframe\n",
    "data = [\n",
    "    ('TXN001', '2023-02-18', 250.75, 'Electronics', 'Bought a new phone'),\n",
    "    ('TXN002', '2024-02-25', -50.50, 'Groceries', 'Refunded groceries'),\n",
    "    ('TXN003', '2019-03-01', 125.00, 'Clothing', 'Purchased new Jacket'),\n",
    "    ('TXN004', '2024-11-28', -10.00, 'Books', 'Refunded Book Purchase'),\n",
    "]\n",
    "\n",
    "# schema definition\n",
    "schema = 'TransactionID String, TransactionDate String, Amount Double, Category String, Description String'\n",
    "\n",
    "# create dataframe\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.display()\n",
    "\n",
    "# transform function\n",
    "def convert_to_date(df):\n",
    "    return df.withColumn('TransactionDate', to_date(col('TransactionDate'), 'yyyy-MM-dd'))\n",
    "\n",
    "def extract_year(df):\n",
    "    return df.withColumn('Year', year(col('TransactionDate')))\n",
    "\n",
    "def extract_month(df):\n",
    "    return df.withColumn('Month', month(col('TransactionDate')))\n",
    "\n",
    "def length_of_description(df):\n",
    "    return df.withColumn('DescriptionLength', length(col('Description')))\n",
    "\n",
    "def absolute_value_of_amount(df):\n",
    "    return df.withColumn('Amount_Abs', abs(col('Amount')))\n",
    "\n",
    "def indicate_if_refund(df):\n",
    "    return df.withColumn('Is_Refund', when(col('Amount') < 0, True).otherwise(False))\n",
    "\n",
    "def last_day_of_month(df):\n",
    "    return df.withColumn('Last_Day_OfMonth', last_day(col('TransactionDate')))\n",
    "\n",
    "def format_date(df) :\n",
    "    return df.withColumn('Formatted_Date', date_format(col('TransactionDate'), 'yyyy-MM'))\n",
    "\n",
    "def category_transaction_size(df):\n",
    "    return df.withColumn('Transaction_Size', when(col('Amount_Abs') > 100, 'Large').otherwise('small'))\n",
    "\n",
    "def dynamic_calculation(df):\n",
    "    return df.withColumn('Dynamic_Calculation',\n",
    "                          expr(\n",
    "                        \"\"\"\n",
    "                            CASE\n",
    "                                WHEN Category = 'Electronics' THEN Amount * 1.10\n",
    "                                WHEN Category = 'Groceries' THEN Amount * 0.90\n",
    "                                ELSE Amount\n",
    "                            END\n",
    "                        \"\"\" \n",
    "                       )\n",
    "    )\n",
    "\n",
    "# apply transformation\n",
    "df = (df\n",
    "      .transform(convert_to_date)\n",
    "      .transform(extract_year)\n",
    "      .transform(extract_month)      \n",
    "      .transform(length_of_description)\n",
    "      .transform(absolute_value_of_amount)\n",
    "      .transform(indicate_if_refund)\n",
    "      .transform(last_day_of_month)\n",
    "      .transform(format_date)\n",
    "      .transform(category_transaction_size)\n",
    "      .transform(dynamic_calculation)\n",
    ")\n",
    "\n",
    "# show the result   \n",
    "df.display()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "896161a8-7362-4ec3-bbf4-acb7151e5769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Complex Example with Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "411506a4-73b2-41a4-9ba3-d471a3964f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, log, when, concat_ws, lit\n",
    "\n",
    "# sample dataframe\n",
    "data = [('Alice', 34), ('Bob', 28), ('Charlie', 42), ('Dave', 50), ('Eve', 22)]\n",
    "\n",
    "df = spark.createDataFrame(data, ['name', 'age'])\n",
    "\n",
    "# parameters\n",
    "age_thershold = 40\n",
    "log_base = 10\n",
    "\n",
    "# define transformation functions with parameters\n",
    "\n",
    "# function to clean data by trimmimg whitespaces from the 'Name' column\n",
    "def clean_data(df):\n",
    "    return df.withColumn('name', trim(col('name')))\n",
    "\n",
    "# function to categeorize 'Age' into 'Young' and 'Mature' based on age_threshold\n",
    "def categorize_age(df, age_threshold):\n",
    "    return df.withColumn('age_category', when(col('age') < lit(age_threshold), 'Young').otherwise('Mature'))\n",
    "\n",
    "# function to apply logarthmic transformation to 'Age' column\n",
    "def log_transform(df, log_base):\n",
    "    return df.withColumn('LogAge', log(col('age')) / log(lit(log_base)))\n",
    "\n",
    "# function to create new column 'Name_Age' by concatenating 'Name' and 'Age'\n",
    "def create_name_age(df):\n",
    "    return df.withColumn('name_age', concat_ws('_', col('name'), col('age')))\n",
    "\n",
    "# apply transformation using transform function with parameters\n",
    "df_transformed = (df.transform(clean_data) # apply data cleaning\n",
    "            .transform(lambda df: categorize_age(df, age_thershold) # apply conditional transformation with threshold\n",
    "                .transform(lambda df: log_transform(df, log_base)) # apply log transformation with base\n",
    "                .transform(create_name_age) # apply name-age transformation\n",
    "                ))\n",
    "\n",
    "df_transformed.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "003bb0c9-328e-4902-8567-b1879e0f996b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Cleansing Use Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b82f2024-a2d3-430d-8aeb-73265f5f4a15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, when, avg, sum as _sum\n",
    "\n",
    "# sample dataframe\n",
    "data =[\n",
    "    ('Alice', 34, '2023-06-01', 3000.0),\n",
    "    ('Bob', 45, '2023-06-02', None),\n",
    "    ('Cathy', None, '2023-06-03', 2500.0),\n",
    "    ('Alice', 34, '2023-06-01', 3000.0),\n",
    "    (None, 45, None, 4000.0),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, ['name', 'age', 'date', 'salary'])\n",
    "\n",
    "# define transformation functions\n",
    "\n",
    "# fill values for null handling\n",
    "name_fill = 'Unknown'\n",
    "age_fill = 0\n",
    "date_fill = '1900-01-01'\n",
    "salary_fill = 0.0\n",
    "age_threshold = 30\n",
    "salary_threshold = 2000\n",
    "bonus_percentage = 0.10\n",
    "\n",
    "# function to fill null values \n",
    "def handle_null(df, name_fill, age_fill, date_fill, salary_fill):\n",
    "    return df.fillna({\n",
    "        'name':name_fill,\n",
    "        'age':age_fill,\n",
    "        'date':date_fill,\n",
    "        'salary':salary_fill\n",
    "    })\n",
    "\n",
    "# function to remove duplicates\n",
    "def remove_duplicates(df):\n",
    "    return df.dropDuplicates()\n",
    "\n",
    "# function to standardize data types\n",
    "def standardize_data_types(df):\n",
    "    return df.withColumn('age', col('age').cast('integer')) \\\n",
    "             .withColumn('date', col('date').cast('date')) \\\n",
    "             .withColumn('salary', col('salary').cast('double'))\n",
    "\n",
    "# function to filter rows based on conditions\n",
    "def filter_rows(df, age_threshold, salary_threshold):\n",
    "    return df.filter((col('age') >= lit(age_threshold)) & (col('salary') >= lit(salary_threshold)))\n",
    "\n",
    "# function to add new calculated colum 'bonus'\n",
    "def add_bonus(df, bonus_percentage):\n",
    "    return df.withColumn('bonus', col('salary') * lit(bonus_percentage))\n",
    "\n",
    "# function to perfrom group by and aggregate results\n",
    "def group_and_aggregate(df):\n",
    "    return df.groupBy('name').agg(\n",
    "        avg('age').alias('avg_age'),\n",
    "        _sum('salary').alias('total_salary'),\n",
    "        _sum('bonus').alias('total_bonus')\n",
    "    )\n",
    "\n",
    "# apply transformation using transform function with parameters\n",
    "df_transformed = (df.transform(lambda df: handle_null(df, name_fill, age_fill, date_fill, salary_fill)) # apply null handling\n",
    "            .transform(remove_duplicates) # apply duplicate removal\n",
    "            .transform(standardize_data_types) # apply data type standardization\n",
    "            .transform(lambda df: filter_rows(df, age_threshold, salary_threshold)) # apply filtering with threshold\n",
    "            .transform(lambda df: add_bonus(df, bonus_percentage)) # apply bonus calculation\n",
    "            .transform(group_and_aggregate) # apply group by and aggregate\n",
    "            )\n",
    "\n",
    "df_transformed.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "110 - Transform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
